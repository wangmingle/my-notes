nltk

## 基础
pip install nltk

```python
import nltk
nltk.download() # book
from nltk.book import *

 text1

 text1.concordance("former")   # 包含former的句子

 text1.similar("ship") # 近意

# 我们还可以查看某个词在文章里出现的位置：
 text4.dispersion_plot(["citizens", "democracy", "freedom", "duties", "America"])
```

sudo pip install matplotlib

```
len(text1)：返回总字数

set(text1)：返回文本的所有词集合

len(set(text4))：返回文本总词数

text4.count("is")：返回“is”这个词出现的总次数

FreqDist(text1)：统计文章的词频并按从大到小排序存到一个列表里

fdist1 = FreqDist(text1);fdist1.plot(50, cumulative=True)：统计词频，并输出累计图像

纵轴表示累加了横轴里的词之后总词数是多少，这样看来，这些词加起来几乎达到了文章的总词数

fdist1.hapaxes()：返回只出现一次的词

text4.collocations()：频繁的双联词
```


echo "backend: TkAgg" > ~/.matplotlib/matplotlibrc

## 语料库

```
nltk.corpus.gutenberg.fileids()
nltk.corpus.gutenberg.raw('chesterton-brown.txt')
输出chesterton-brown.txt文章的原始内容

nltk.corpus.gutenberg.words('chesterton-brown.txt')
输出chesterton-brown.txt文章的单词列表

nltk.corpus.gutenberg.sents('chesterton-brown.txt')
输出chesterton-brown.txt文章的句子列表

from nltk.corpus import webtext
网络文本语料库，网络和聊天文本

from nltk.corpus import brown
布朗语料库，按照文本分类好的500个不同来源的文本

from nltk.corpus import reuters
路透社语料库，1万多个新闻文档

from nltk.corpus import inaugural
就职演说语料库，55个总统的演说
```

```
from imp import reload
# coding:utf-8

import sys
reload(sys)
# sys.setdefaultencoding( "utf-8" )

import nltk
from nltk.corpus import brown

# 链表推导式， genre 是 brown 语料库里的所有类别列表，word是这个类别中的词汇列表
# (genre, word)就是类别加词汇对
genre_word = [(genre, word)
        for genre in brown.categories()
        for word in brown.words(categories=genre)
        ]

# 创建条件频率分布
cfd = nltk.ConditionalFreqDist(genre_word)

# 指定条件和样本作图
cfd.plot(conditions=['news','adventure'], samples=[u'stock', u'sunbonnet', u'Elevated', u'narcotic', u'four', u'woods', u'railing', u'Until', u'aggression', u'marching', u'looking', u'eligible', u'electricity', u'$25-a-plate', u'consulate', u'Casey', u'all-county', u'Belgians', u'Western', u'1959-60', u'Duhagon', u'sinking', u'1,119', u'co-operation', u'Famed', u'regional', u'Charitable', u'appropriation', u'yellow', u'uncertain', u'Heights', u'bringing', u'prize', u'Loen', u'Publique', u'wooden', u'Loeb', u'963', u'specialties', u'Sands', u'succession', u'Paul', u'Phyfe'])
```

```
# 词干
>>> import nltk
>>> porter = nltk.PorterStemmer()
>>> porter.stem('lying')
u'lie'
# 词性
>>> import nltk
>>> text = nltk.word_tokenize("And now for something completely different")
>>> nltk.pos_tag(text)
[('And', 'CC'), ('now', 'RB'), ('for', 'IN'), ('something', 'NN'), ('completely', 'RB'), ('different', 'JJ')]
```


```
from imp import reload
import sys
reload(sys)
import nltk

for word in nltk.corpus.sinica_treebank.tagged_words():
    print (word[0], word[1])
```

## 贝叶斯
from imp import reload
import sys
reload(sys)
import nltk
my_train_set = [
        ({'feature1':u'a'},'1'),
        ({'feature1':u'a'},'2'),
        ({'feature1':u'a'},'3'),
        ({'feature1':u'a'},'3'),
        ({'feature1':u'b'},'2'),
        ({'feature1':u'b'},'2'),
        ({'feature1':u'b'},'2'),
        ({'feature1':u'b'},'2'),
        ({'feature1':u'b'},'2'),
        ({'feature1':u'b'},'2'),
        ]
classifier = nltk.NaiveBayesClassifier.train(my_train_set)
print(classifier.classify({'feature1':u'a'}))
print(classifier.classify({'feature1':u'b'}))

海量文本知识表示
  网络文本资源获取
  机器学习方法
  大规模语义计算和推理
  知识表示体系
  知识库构建

问句解析
  中文分词
  词性标注
  实体标注
  概念类别标注
  句法分析
  语义分析
  逻辑结构标注
  指代消解
  关联关系标注
  问句分类（简单问句还是复杂问句 实体型还是段落型还是篇章级问题）
  答案类别确定

答案生成与过滤
  候选答案抽取
  关系推演（并列关系还是递进关系还是因果关系）
  吻合程度判断
  噪声过滤


pip install pynlpir

import sys
reload(sys)

import pynlpir

pynlpir.open()
s = '聊天机器人到底该怎么做呢？'
segments = pynlpir.segment(s)
for segment in segments:
    print(segment[0], '\t', segment[1])

pynlpir.close()












